<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bridging Detection and Re-identification Research</title>        
    <!-- ðŸŒ Open Graph Metadata for Social Sharing -->
    <meta property="og:title" content="Bridging Detection and Re-identification">
    <meta property="og:description" content="Face recognition systems typically rely on a two-stage pipeline â€“ face detection and re-identification (ReID) â€“ yet existing evaluations often overlook how detection errors propagate to affect recognition accuracy. This work empirically analyzes the interplay between detection and ReID, proposing a holistic framework to quantify synergy and error propagation.">
    <meta property="og:image" content="https://raw.githubusercontent.com/kuanyewleong/detection-to-reid-benchmark-and-dataset/refs/heads/main/page_media/figure1.png">
    <meta property="og:url" content="https://kuanyewleong.github.io/detection-to-reid-benchmark-and-dataset/">
    <meta property="og:type" content="website">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;700&family=Inter:wght@300;400;600;700&display=swap');

        :root {
            --primary-color: #00ABE4;
            --secondary-color: #005A87;
            --background-color: #FFFFFF;
            --text-color: #212121;
            --subtle-color: #757575;
            --border-color: #E9F1FA;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.75;
            color: var(--text-color);
            max-width: 900px;
            margin: 0 auto;
            padding: 30px;
            background-color: var(--background-color);
        }

        header {
            text-align: center;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 2rem;
        }

        h1 {
            font-family: 'Roboto Mono', monospace;
            font-size: 2rem;
            color: var(--primary-color);
            margin-bottom: 0.75rem;
        }

        h2 {
            font-family: 'Roboto Mono', monospace;
            font-size: 1.5rem;
            color: var(--secondary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .authors {
            font-size: 1rem;
            color: var(--subtle-color);
        }

        .affiliation {
            font-style: italic;
            color: var(--subtle-color);
        }

        .abstract {
            background-color: #E9F1FA;
            padding: 1rem;
            border-left: 4px solid var(--primary-color);
            margin: 2rem 0;
            text-align: justify;
            font-size: 0.95rem;
        }

        .figure {
            margin: 2rem 0;
            text-align: center;
        }

        .figure img {
            max-width: 100%;
            border: 1px solid var(--border-color);
        }

        .figure-caption {
            font-family: 'Roboto Mono', monospace;
            font-size: 0.9rem;
            margin-top: 0.75rem;
            color: var(--subtle-color);
        }

        footer {
            margin-top: 3rem;
            text-align: left;
            padding-top: 1rem;
            border-top: 1px solid var(--border-color);
            font-size: 0.85rem;
            color: var(--subtle-color);
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            border-bottom: 1px dotted var(--primary-color);
            transition: color 0.2s ease;
        }

        a:hover {
            color: var(--secondary-color);
            border-bottom: 1px solid var(--secondary-color);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.75rem;
            }

            body {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Bridging Detection and Re-identification: Evaluating Trustworthiness and Error Propagation in Face Recognition Pipelines</h1>        
        
        <div class="authors">
            <strong>Kuan Yew Leong and Jaeseung Han</strong>
        </div>
        
        <div class="affiliation">
            A.I. System Research Co. Ltd.<br>
            Kyoto, 606-8302 Japan<br>
            {kyleong, han}@aisystemresearch.com
        </div>
        
        <div class="contact">
            <strong>Corresponding author:</strong> <a href="mailto:kuanyew.leong@gmail.com">kuanyew.leong@gmail.com</a>
        </div>

        <p>(This work is accepted for presentation at CVPR 2025 Workshops. The final published version of the proceedings will be available on IEEE Xplore.)</p>
    </header>
    
    <main>
        <section>
            <h2>Abstract</h2>
            <div class="abstract">
                <p>Face recognition systems typically rely on a two-stage pipeline â€“ face detection and re-identification (ReID) â€“ yet existing evaluations often overlook how detection errors propagate to affect recognition accuracy. This work empirically analyzes the interplay between detection and ReID, proposing a holistic framework to quantify synergy and error propagation.</p>
                
                <p>Grounded in information entropy, we introduce the Detection-Recognition Synergy (DRS) Score, a composite metric integrating Mutual Information (MI), Jensen-Shannon Divergence (JSD), and Wasserstein Distance (WD) to assess detection's impact on recognition. We construct a novel benchmark dataset, annotated for both detection and ReID, featuring temporal sequences of the same individuals, diverse backgrounds, and natural settings across both indoor and outdoor environments.</p>
                
                <p>Experiments with diverse detection models, including YOLOX, Faster R-CNN, SSD, MTCNN and several others reveal substantial performance shifts due to detection variations. Our findings advocate for integrated evaluation strategies to enhance trustworthiness in face recognition pipelines. By bridging detection and ReID assessments, this study sets a foundation for more robust, explainable, and trusted face recognition pipelines.</p>
            </div>
        </section>
        
        <section class="figure">
            <img src="https://raw.githubusercontent.com/kuanyewleong/detection-to-reid-benchmark-and-dataset/refs/heads/main/page_media/figure1.png" alt="Face detection models comparison">
            <div class="figure-caption">
                <strong>Figure 1</strong>: A face was identified and localized from the same image frame by several different detection models. From left: ATSS-r50-fpn, FCOS-MobileNetV2, SSD-MobileNetV2, YOLOX-L, YOLOX-tiny. As shown in the figure, each model positioned its bounding box slightly differently, highlighting variations in detection. These discrepancies can ultimately affect ReID performance, as demonstrated in our experiments. The face is anonymized using attribute-preserving technique due to ethical concerns.
            </div>
        </section>
        <hr>
        <div class="github-link">
            -> View the <strong>full preprint paper</strong>: <a href="https://kuanyewleong.github.io/detection-to-reid-benchmark-and-dataset/page_media/preprint_version.pdf" target="_blank">Bridging Detection and Re-identification - preprint version</a>
            <p>This work has been submitted to the IEEE/CVF for publication. Copyright may be transferred without notice, after which this version may no longer be accessible. The final published version of the proceedings will be available on IEEE Xplore.</p>
            <br>
        </div>
        <div class="github-link">
            <H3>Technical Documentation of Dataset</H3>            
            <p>The evaluation dataset was meticulously curated from publicly accessible YouTube videos featuring individual faces from diverse backgrounds worldwide, and currently comprises 100 sets of videos (about 25 seconds duration each), primarily containing at least one individual, though some feature multiple individuals; half are categorized for indoor ambience, and the other half for outdoor environments. The generation process involved several key steps:</p>
            <p><strong>Video Selection:</strong> We identified and selected videos showcasing individuals from various ethnicities and cultures to ensure a comprehensive representation of global diversity.</p>
            <p><strong>Timestamping and Clip Extraction:</strong> Specific segments within these videos were marked based on their relevance, and frames were sampled from these segments for further analysis.</p>
            <p><strong>Annotation:</strong> A total of 1,017 diverse instances were annotated, with each unique face marked with bounding boxes and assigned an identifier (ID). 
            </p>
            <br>
        </div>

        <!-- <div class="github-link">
            -> View this project at: <a href="https://github.com/kuanyewleong/detection-to-reid-benchmark-and-dataset" target="_blank">https://github.com/kuanyewleong/detection-to-reid-benchmark-and-dataset</a>
        </div> -->
    </main>
    
    <footer>
        <p>Cite this work as:</p>
        <p>K. Y. Leong and J. Han, "Bridging Detection and Re-identification: Evaluating Trustworthiness and Error Propagation in Face Recognition Pipelines," in Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Nashville, TN, USA, 2025.</p>
    </footer>
</body>
</html>